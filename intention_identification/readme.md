# 对话意图识别项目

目的：基于给定文本，判断该文本对应的意图信息是什么，方便后续任务进行数据处理。
步骤：

1. 数据整理
   1. 提取意图种类
   2. 借鉴torchtext简化数据加载相关代码
   3. token处理（词、字、笔化特征）
   4. 词表处理（停止词、填充词、未知词）
2. 构造数据加载器
   1. 构建dataset
   2. 构建dataloader
3. 模型构建
   1. RNN + FC
   2. LSTM + FC
   3. GRU + FC
   4. 代码提炼优化
4. 构建优化器
   1. 学习率怎么调
   2. 优化器
5. 模型训练、评估
   1. Trainer
   2. Evaluator
6. 可视化机制：基于TensorFlow的tensorboard来实现
7. 训练方式
   1. 直接设置学习率=0.1, 惩罚性系数=0.01
      模型训练不收敛，产生原因：学习率太大了，前期加入的惩罚性系数有点限制模型的收敛。
   2. 学习率=0.01， 惩罚性系数=0.0
      过拟合、并且随着训练次数的增加，训练集上的效果变成呈现: 先变好再变差的情况产生原样：学习率过大，会跳过全局最优解
      优点：前期可以快速逼近全局最优解
      日志: log_lr0.01_weightdecay0.0_01_新训练.log
   3. 两阶段的训练：
      第一阶段：学习率=0.001, 惩罚性系数=0.0, 学习65个epoch后
      第二阶段：学习率=0.001, 惩罚性系数=0.01, 学习5个epoch即可
      日志: log_lr0.001_weightdecay0.0_02_新训练.log、log_lr0.001_weightdecay0.01_03_基于02的基础上训练.log
8. 模型部署

NOTE:
1. 大的学习率可以让模型前期快速逼近全局最优解，但是后期可能会导致模型不收敛；
2. 小的学习率更新的步长比较小的，逼近全局最优解的速度是比较慢，但是在后期的时候，可以让模型缓慢的收敛到最优解位置；
3. 惩罚项可以防止模型过拟合、增加训练数据也可以防止模型过拟合(普通的增加数据、数据增强)；  

扩展：数据增强
1. 更改一下文本x的token顺序
2. Mask Token操作：随机将部分token进行掩盖操作
3. 同义词替换、同实体词替换、同音词的替换、拼音替换等等