目的：实现Word2Vec的训练模型，并产生最终的词向量embedding_table数据，方便其它NLP模型引用
步骤：

1. 数据解析处理

   1. 文本分词：将文本转换为单词
   2. 构建一个单词到id的映射表（构建一个完整的词典对象）
   3. 构建训练数据（与模型相关）窗口大小决定训练数据的长度

      窗口大小固定为5(1中心词 + 4周边词)
2. 模型结构构建

   1. CBOW
   2. SkipGram
   3. 负采样：当上面两个模型的vocab规模较大且计算资源有限时，训练过程会受输出层概率归一化计算效率的影响。负采样将问题简化为对于(w, c)d的二元分类问题（共现或者非共现），规避了大词表的归一化计算。
3. 训练

   1. 加载恢复已有模型
   2. 训练单个epoch
   3. 定时保存模型
